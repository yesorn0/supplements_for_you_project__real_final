import pickle
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.io import mmwrite
import os
import logging
from pathlib import Path
import time
from typing import Tuple, Optional
import warnings

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings('ignore')


class TFIDFProcessor:
    """TF-IDF ë²¡í„°í™” ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""

    def __init__(self,
                 input_path: str = './cleaned_data/cleaned_supplements.csv',
                 output_dir: str = './models',
                 log_level: str = 'INFO'):
        """
        ì´ˆê¸°í™”

        Args:
            input_path: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            log_level: ë¡œê·¸ ë ˆë²¨ ('DEBUG', 'INFO', 'WARNING', 'ERROR')
        """
        self.input_path = input_path
        self.output_dir = Path(output_dir)
        self.df_review = None
        self.df_grouped = None
        self.tfidf = None
        self.tfidf_matrix = None

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ë¡œê¹… ì„¤ì •
        self._setup_logging(log_level)

    def _setup_logging(self, log_level: str) -> None:
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'tfidf_process.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        self.logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {self.input_path}")

        try:
            df = pd.read_csv(self.input_path)
            self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ í–‰")

            # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
            required_cols = ['review', 'product', 'ingredient', 'url']
            missing_cols = [col for col in required_cols if col not in df.columns]

            if missing_cols:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")

            return df

        except FileNotFoundError:
            self.logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.input_path}")
            raise
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì •ì œ"""
        self.logger.info("ğŸ§¹ ë°ì´í„° ì •ì œ ì¤‘...")

        initial_count = len(df)

        # ê²°ì¸¡ì¹˜ ì œê±° (ë” íš¨ìœ¨ì ì¸ ë°©ë²•)
        required_cols = ['review', 'product', 'ingredient', 'url']
        df_clean = df.dropna(subset=required_cols)

        # ë¹ˆ ë¬¸ìì—´ ì œê±°
        for col in required_cols:
            df_clean = df_clean[df_clean[col].str.strip() != '']

        # ì¤‘ë³µ ì œê±° (ì œí’ˆëª…+ë¦¬ë·° ê¸°ì¤€)
        df_clean = df_clean.drop_duplicates(subset=['product', 'review'])

        removed_count = initial_count - len(df_clean)
        self.logger.info(f"âœ… ì •ì œ ì™„ë£Œ: {removed_count:,}ê°œ í–‰ ì œê±°, {len(df_clean):,}ê°œ í–‰ ìœ ì§€")

        return df_clean

    def group_reviews(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì œí’ˆë³„ ë¦¬ë·° ê·¸ë£¹í™”"""
        self.logger.info("ğŸ“Š ì œí’ˆë³„ ë¦¬ë·° ê·¸ë£¹í™” ì¤‘...")
        start_time = time.time()

        # ì œí’ˆë³„ í†µê³„ ì •ë³´ ì¶”ê°€
        product_stats = df.groupby('product').agg({
            'review': ['count', lambda x: ' '.join(x)],
            'ingredient': 'first',
            'url': 'first'
        }).reset_index()

        # ì»¬ëŸ¼ëª… ì •ë¦¬
        product_stats.columns = ['product', 'review_count', 'review', 'ingredient', 'url']

        # ë¦¬ë·° ê¸¸ì´ ì •ë³´ ì¶”ê°€
        product_stats['review_length'] = product_stats['review'].str.len()
        product_stats['avg_review_length'] = product_stats['review_length'] / product_stats['review_count']

        elapsed_time = time.time() - start_time
        self.logger.info(f"âœ… ê·¸ë£¹í™” ì™„ë£Œ: {len(product_stats):,}ê°œ ì œí’ˆ, {elapsed_time:.2f}ì´ˆ ì†Œìš”")

        return product_stats

    def create_tfidf_matrix(self, df: pd.DataFrame, **tfidf_params) -> Tuple[TfidfVectorizer, any]:
        """TF-IDF í–‰ë ¬ ìƒì„±"""
        self.logger.info("ğŸ”¢ TF-IDF ë²¡í„°í™” ì¤‘...")
        start_time = time.time()

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        default_params = {
            'sublinear_tf': True,
            'max_features': 10000,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì œí•œ
            'min_df': 2,  # ìµœì†Œ 2ë²ˆ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ
            'max_df': 0.8,  # 80% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ì œì™¸
            'ngram_range': (1, 2)  # 1-gram, 2-gram ëª¨ë‘ ì‚¬ìš©
        }
        default_params.update(tfidf_params)

        try:
            tfidf = TfidfVectorizer(**default_params)
            tfidf_matrix = tfidf.fit_transform(df['review'])

            elapsed_time = time.time() - start_time

            # í–‰ë ¬ ì •ë³´ ì¶œë ¥
            self.logger.info(f"âœ… TF-IDF ì™„ë£Œ:")
            self.logger.info(f"   - í–‰ë ¬ í¬ê¸°: {tfidf_matrix.shape}")
            self.logger.info(f"   - íŠ¹ì„± ìˆ˜: {len(tfidf.get_feature_names_out()):,}")
            self.logger.info(
                f"   - í¬ì†Œì„±: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%")
            self.logger.info(f"   - ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

            return tfidf, tfidf_matrix

        except Exception as e:
            self.logger.error(f"âŒ TF-IDF ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            raise

    def save_results(self, df: pd.DataFrame, tfidf: TfidfVectorizer, tfidf_matrix) -> None:
        """ê²°ê³¼ ì €ì¥"""
        self.logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")

        try:
            # 1. TF-IDF ëª¨ë¸ ì €ì¥
            with open(self.output_dir / 'tfidf.pickle', 'wb') as f:
                pickle.dump(tfidf, f)
            self.logger.info("âœ… TF-IDF ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

            # 2. TF-IDF í–‰ë ¬ ì €ì¥ (Matrix Market í˜•ì‹)
            mmwrite(str(self.output_dir / 'tfidf_supplements.mtx'), tfidf_matrix)
            self.logger.info("âœ… TF-IDF í–‰ë ¬ ì €ì¥ ì™„ë£Œ")

            # 3. ì œí’ˆ ì •ë³´ ì €ì¥ (í†µê³„ ì •ë³´ í¬í•¨)
            output_cols = ['product', 'review', 'ingredient', 'url', 'review_count', 'review_length',
                           'avg_review_length']
            df[output_cols].to_csv(
                self.output_dir / 'tfidf_products.csv',
                index=False,
                encoding='utf-8-sig'
            )
            self.logger.info("âœ… ì œí’ˆ ì •ë³´ ì €ì¥ ì™„ë£Œ")

            # 4. íŠ¹ì„±(ë‹¨ì–´) ëª©ë¡ ì €ì¥
            feature_names = tfidf.get_feature_names_out()
            pd.DataFrame({'feature': feature_names}).to_csv(
                self.output_dir / 'tfidf_features.csv',
                index=False,
                encoding='utf-8-sig'
            )
            self.logger.info("âœ… íŠ¹ì„± ëª©ë¡ ì €ì¥ ì™„ë£Œ")

            # 5. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'total_products': len(df),
                'matrix_shape': tfidf_matrix.shape,
                'feature_count': len(feature_names),
                'sparsity': (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100,
                'processing_date': pd.Timestamp.now().isoformat()
            }

            pd.DataFrame([metadata]).to_csv(
                self.output_dir / 'tfidf_metadata.csv',
                index=False
            )
            self.logger.info("âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def generate_report(self, df: pd.DataFrame) -> None:
        """ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("\n" + "=" * 50)
        self.logger.info("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        self.logger.info("=" * 50)

        # ê¸°ë³¸ í†µê³„
        self.logger.info(f"ì´ ì œí’ˆ ìˆ˜: {len(df):,}")
        self.logger.info(f"ì´ ë¦¬ë·° ìˆ˜: {df['review_count'].sum():,}")
        self.logger.info(f"í‰ê·  ì œí’ˆë‹¹ ë¦¬ë·° ìˆ˜: {df['review_count'].mean():.1f}")

        # ë¦¬ë·° ê¸¸ì´ í†µê³„
        self.logger.info(f"í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['avg_review_length'].mean():.0f}ì")
        self.logger.info(f"ìµœì¥ ë¦¬ë·° ê¸¸ì´: {df['review_length'].max():,}ì")

        # ìƒìœ„ ì œí’ˆ (ë¦¬ë·° ìˆ˜ ê¸°ì¤€)
        top_products = df.nlargest(5, 'review_count')[['product', 'review_count']]
        self.logger.info("\nğŸ† ë¦¬ë·° ìˆ˜ ìƒìœ„ 5ê°œ ì œí’ˆ:")
        for _, row in top_products.iterrows():
            self.logger.info(f"   {row['product']}: {row['review_count']}ê°œ")

        self.logger.info("=" * 50)

    def process(self, **tfidf_params) -> None:
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        total_start_time = time.time()

        try:
            # 1. ë°ì´í„° ë¡œë“œ
            self.df_review = self.load_data()

            # 2. ë°ì´í„° ì •ì œ
            self.df_review = self.clean_data(self.df_review)

            # 3. ë¦¬ë·° ê·¸ë£¹í™”
            self.df_grouped = self.group_reviews(self.df_review)

            # 4. TF-IDF ë²¡í„°í™”
            self.tfidf, self.tfidf_matrix = self.create_tfidf_matrix(self.df_grouped, **tfidf_params)

            # 5. ê²°ê³¼ ì €ì¥
            self.save_results(self.df_grouped, self.tfidf, self.tfidf_matrix)

            # 6. ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_report(self.df_grouped)

            total_elapsed_time = time.time() - total_start_time
            self.logger.info(f"\nğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")

        except Exception as e:
            self.logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # TF-IDF íŒŒë¼ë¯¸í„° ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
    tfidf_params = {
        'max_features': 15000,
        'min_df': 3,
        'max_df': 0.7,
        'ngram_range': (1, 2)
    }

    processor = TFIDFProcessor(
        input_path='./cleaned_data/cleaned_supplements.csv',
        output_dir='./models',
        log_level='INFO'
    )

    processor.process(**tfidf_params)


if __name__ == "__main__":
    main()