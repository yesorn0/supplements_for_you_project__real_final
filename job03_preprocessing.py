import pandas as pd
import re
from konlpy.tag import Okt
import time
from multiprocessing import Pool, cpu_count
import numpy as np

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('./cleaned_data/all_supplements.csv')
print(df.info())
print(df.head())

# 0ë²ˆì§¸ ì—´: ì˜ì–‘ì œ ì¢…ë¥˜ ì •ì œ (ë„ì–´ì“°ê¸° ì œê±° + ëª…ì¹­ í†µì¼)
df.iloc[:, 0] = df.iloc[:, 0].str.replace(' ', '', regex=False)  # ëª¨ë“  ê³µë°± ì œê±°

# ëª…ì¹­ í†µì¼
df.iloc[:, 0] = df.iloc[:, 0].replace({
    'ë‚¨ì„±ìš©ë¹„íƒ€ë¯¼': 'ë‚¨ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ë‚¨ì„±ìš©ì¢…í•©ë¹„íƒ€ë¯¼': 'ë‚¨ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ì—¬ì„±ìš©ë¹„íƒ€ë¯¼': 'ì—¬ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ì—¬ì„±ìš©ì¢…í•©ë¹„íƒ€ë¯¼': 'ì—¬ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ë©€í‹°ë¹„íƒ€ë¯¼': 'ì¢…í•©ë¹„íƒ€ë¯¼',
    'ë‚¨ì„±ë©€í‹°ë¹„íƒ€ë¯¼': 'ë‚¨ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ì—¬ì„±ë©€í‹°ë¹„íƒ€ë¯¼': 'ì—¬ì„±ì¢…í•©ë¹„íƒ€ë¯¼',
    'ì„ì‚°ë¶€ì¢…í•©ë¹„íƒ€ë¯¼': 'ì„ì‚°ë¶€ì¢…í•©ë¹„íƒ€ë¯¼',
})

print("\nâœ… ì •ì œëœ ì˜ì–‘ì œ ì¢…ë¥˜ ê³ ìœ ê°’:")
print(df.iloc[:, 0].unique())

# 2. ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´ ìˆ˜: {cpu_count()}")

# 3. ë¶ˆìš©ì–´ë¥¼ setìœ¼ë¡œ ë³€í™˜ (ê²€ìƒ‰ ì†ë„ O(1))
stop_words = set([
    # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤', 'ë¨¹ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤',
    'ë§ë‹¤', 'ì ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤', 'ë“¤ë‹¤', 'ê¾¸ì¤€í•˜ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'íƒì›”í•˜ë‹¤',
    'ë§Œì¡±í•˜ë‹¤', 'ë„˜ë‹¤', 'ë“¤ì–´ì„œë‹¤', 'ì•ˆë˜ë‹¤', 'ì¢‹ì•„í•˜ë‹¤', 'ë˜ì–´ë‹¤', 'ê¹”ë”í•˜ë‹¤', 'í•´ë³´ë‹¤',
    'í•´ì£¼ë‹¤', 'ë¨¹ì´ë‹¤', 'ë¨¹ì–´ì£¼ë‹¤', 'ì•„ë‹ˆë‹¤', 'ì¬ë‹¤', 'ë„˜ì–´ê°€ë‹¤', 'ì‚¬ë‹¤', 'ë¹ ë¥´ë‹¤',
    'ì¼œì§€ë‹¤', 'ë‹¤ë§Œ', 'ë¹„ì‹¸ë‹¤', 'ê¸°ë‹¤ë¦¬ë‹¤', 'íŠ¹ë³„í•˜ë‹¤', 'ê¼¼ê¼¼í•˜ë‹¤', 'ì´ë‹¤', 'ì–´ë µë‹¤',
    'ì•Šë‹¤', 'ë¨¹ê¸°', 'ë¶€ë¶„', 'ì±™ê¸°ë‹¤', 'ë§›ìˆë‹¤',
    # ëª…ì‚¬
    'ì œí’ˆ', 'ì˜ì–‘ì œ', 'êµ¬ë§¤', 'êµ¬ì…', 'ì‚¬ìš©', 'ê°€ê²©', 'í¬ì¥', 'ë°°ì†¡', 'ë¦¬ë·°',
    'í›„ê¸°', 'í’ˆì ˆ', 'ìœ ê¸°ë†', 'ì§êµ¬', 'í•¨ëŸ‰', 'ëƒ„ìƒˆ', 'ë¶€ë‹´', 'êµ¬ë¯¸', 'ì ¤ë¦¬',
    # ìˆ˜ëŸ‰, ì‹œê°„, ê¸°íƒ€
    'í•˜ë£¨', 'ê°œì›”', 'ì´ìƒ', 'ì •ë„', 'ì „', 'í›„', 'ì§€ê¸ˆ', 'ì¡°ê¸ˆ', 'ë‹¤ì‹œ', 'ê³„ì†',
    'ì²˜ìŒ', 'ìµœê·¼', 'í˜„ì¬', 'ë§¤ì¼', 'ëŠ˜', 'í•¨ê»˜',
    # ì‹ ì²´/ê°ì • ì¶”ìƒì–´
    'ëŠë‚Œ', 'ê¸°ë¶„', 'ë§Œì¡±', 'ë¶ˆë§Œ',
])

# 4. preserve_wordsë„ setìœ¼ë¡œ ë³€í™˜
preserve_words = set(['A', 'B', 'C', 'D', 'E', 'K', 'ì†', 'ë°œ', 'ëª©', 'ëª¸', 'ëˆˆ',
                      'íŒ”', 'ê°„', 'ì¥', 'ë‡Œ', 'ë¼ˆ', 'ê·€', 'ì½”', 'ìœ„', 'í', 'í”¼'])

# 5. ì •ê·œí‘œí˜„ì‹ ì»´íŒŒì¼ (ì„±ëŠ¥ í–¥ìƒ)
vitamin_patterns = [
    (re.compile(r'ë¹„íƒ€ë¯¼\s*([AaBbCcDdEeKk])', re.IGNORECASE), r'ë¹„íƒ€ë¯¼\1'),
    (re.compile(r'ë¹„íƒ€ë¯¼([a-z])'), lambda m: f'ë¹„íƒ€ë¯¼{m.group(1).upper()}'),
    (re.compile(r'ë¹„íƒ€ë¯¼\s*[Bb]\s*(\d+)'), r'ë¹„íƒ€ë¯¼B\1'),
    (re.compile(r'[Vv]itamin\s*([AaBbCcDdEeKk])'), r'ë¹„íƒ€ë¯¼\1'),
    (re.compile(r'ë¹„íƒ€ë¯¼\s*ë””\s*(?:3|three)', re.IGNORECASE), 'ë¹„íƒ€ë¯¼D'),
    (re.compile(r'ë¹„íƒ€ë¯¼\s*ì”¨', re.IGNORECASE), 'ë¹„íƒ€ë¯¼C'),
]

special_char_pattern = re.compile('[^ê°€-í£A-Za-z0-9]')


def normalize_vitamins_optimized(text):
    """ìµœì í™”ëœ ë¹„íƒ€ë¯¼ í‘œê¸° ì •ê·œí™” í•¨ìˆ˜"""
    if pd.isna(text) or text == '':
        return text

    text = str(text)

    # ì»´íŒŒì¼ëœ ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©
    for pattern, replacement in vitamin_patterns:
        if callable(replacement):
            text = pattern.sub(replacement, text)
        else:
            text = pattern.sub(replacement, text)

    return text


def process_single_review(review_data):
    """ë‹¨ì¼ ë¦¬ë·° ì²˜ë¦¬ í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    idx, review = review_data

    if pd.isna(review):
        return idx, ''

    try:
        review = str(review)
        review = review.replace('|', ' ')

        # ë¹„íƒ€ë¯¼ í‘œê¸° ì •ê·œí™” (lower() ì „ì—)
        review = normalize_vitamins_optimized(review)

        review = review.lower()

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
        review = special_char_pattern.sub(' ', review)

        # ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if not review.strip():
            return idx, ''

        # í˜•íƒœì†Œ ë¶„ì„
        okt = Okt()  # ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤
        tokens = okt.pos(review, stem=True)

        words = []
        for w, cls in tokens:
            if w in preserve_words:
                words.append(w)
            elif cls in ['Noun', 'Adjective', 'Verb'] and len(w) > 1 and w not in stop_words:
                words.append(w)

        cleaned = ' '.join(words)
        return idx, cleaned

    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ (ì¸ë±ìŠ¤ {idx}): {e}")
        return idx, ''


def process_reviews_batch(reviews_batch):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¦¬ë·° ì²˜ë¦¬"""
    results = []
    okt = Okt()  # ë°°ì¹˜ë‹¹ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤

    for idx, review in reviews_batch:
        if pd.isna(review):
            results.append((idx, ''))
            continue

        try:
            review = str(review)
            review = review.replace('|', ' ')

            # ë¹„íƒ€ë¯¼ í‘œê¸° ì •ê·œí™”
            review = normalize_vitamins_optimized(review)

            review = review.lower()
            review = special_char_pattern.sub(' ', review)

            if not review.strip():
                results.append((idx, ''))
                continue

            tokens = okt.pos(review, stem=True)

            words = []
            for w, cls in tokens:
                if w in preserve_words:
                    words.append(w)
                elif cls in ['Noun', 'Adjective', 'Verb'] and len(w) > 1 and w not in stop_words:
                    words.append(w)

            cleaned = ' '.join(words)
            results.append((idx, cleaned))

            # ì§„í–‰ìƒí™© ì¶œë ¥
            if idx % 50 == 0:
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... {idx}")
                print(f"ê²°ê³¼: {cleaned}")

        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ (ì¸ë±ìŠ¤ {idx}): {e}")
            results.append((idx, ''))

    return results


# 6. ë©”ì¸ ì²˜ë¦¬ ë¡œì§
print(f"\nğŸš€ ì´ {len(df)} ê°œì˜ ë¦¬ë·° ì²˜ë¦¬ ì‹œì‘...")
start_time = time.time()

# ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ì˜ ê· í˜•)
batch_size = 50
total_reviews = len(df)
batches = []

# ë°°ì¹˜ ìƒì„±
for i in range(0, total_reviews, batch_size):
    batch = [(idx, review) for idx, review in enumerate(df['review'].iloc[i:i + batch_size], i)]
    batches.append(batch)

print(f"ì´ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")

# ë°°ì¹˜ë³„ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ì‹± ëŒ€ì‹  ìˆœì°¨ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´)
all_results = []
for batch_idx, batch in enumerate(batches):
    print(f"\në°°ì¹˜ {batch_idx + 1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
    batch_results = process_reviews_batch(batch)
    all_results.extend(batch_results)

    # ì¤‘ê°„ ì§„í–‰ë¥  í‘œì‹œ
    progress = (batch_idx + 1) / len(batches) * 100
    elapsed_time = time.time() - start_time
    print(f"ì§„í–‰ë¥ : {progress:.1f}% | ê²½ê³¼ì‹œê°„: {elapsed_time:.1f}ì´ˆ")

# ê²°ê³¼ ì •ë ¬ ë° ì ìš©
all_results.sort(key=lambda x: x[0])  # ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
cleaned_sentences = [result[1] for result in all_results]

# 7. ì €ì¥
df['review'] = cleaned_sentences
print(f"\nì¤‘ë³µ ì œê±° ì „: {len(df)}ê°œ")
df.drop_duplicates(subset=['review'], inplace=True)
print(f"ì¤‘ë³µ ì œê±° í›„: {len(df)}ê°œ")

df.to_csv('./cleaned_data/cleaned_supplements.csv', index=False, encoding='utf-8-sig')

end_time = time.time()
total_time = end_time - start_time
print(f"\nâœ… ë¦¬ë·° ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ")
print(f"í‰ê·  ì²˜ë¦¬ì†ë„: {len(df) / total_time:.2f}ê°œ/ì´ˆ")

# 8. ì •ê·œí™” ê²°ê³¼ í™•ì¸
print("\nâœ… ë¹„íƒ€ë¯¼ í‘œê¸° ì •ê·œí™” í…ŒìŠ¤íŠ¸:")
test_cases = [
    "ë¹„íƒ€ë¯¼ aê°€ ì¢‹ì•„ìš”",
    "ë¹„íƒ€ë¯¼ A íš¨ê³¼",
    "vitamin c ì¶”ì²œ",
    "ë¹„íƒ€ë¯¼ ë””3",
    "ë¹„íƒ€ë¯¼ ì”¨",
    "ë¹„íƒ€ë¯¼B 12",
    "ë¹„íƒ€ë¯¼ b1 ë³µí•©ì²´"
]

for test in test_cases:
    normalized = normalize_vitamins_optimized(test)
    print(f"'{test}' â†’ '{normalized}'")