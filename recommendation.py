import pandas as pd
import re
from konlpy.tag import Okt
from gensim.models import Word2Vec
from functools import lru_cache


# ------------------- í•˜ì´ë¸Œë¦¬ë“œ í† í°í™” -------------------
class HybridTokenizer:
    def __init__(self):
        self.okt = Okt()
        self.pattern = re.compile(r'[^ê°€-í£a-zA-Z0-9]')

    @lru_cache(maxsize=1000)
    def tokenize_smart(self, text):
        """ì§§ì€ í…ìŠ¤íŠ¸ëŠ” í˜•íƒœì†Œ ë¶„ì„, ê¸´ í…ìŠ¤íŠ¸ëŠ” ë„ì–´ì“°ê¸°"""
        if pd.isna(text) or text == '':
            return frozenset()

        text = str(text)

        # ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬ ë°©ë²• ê²°ì •
        if len(text) < 100:  # ì§§ì€ í…ìŠ¤íŠ¸ (ì œí’ˆëª… ë“±)
            cleaned = self.pattern.sub(' ', text)
            return frozenset(self.okt.nouns(cleaned))
        else:  # ê¸´ í…ìŠ¤íŠ¸ (ë¦¬ë·° ë“±)
            # ë„ì–´ì“°ê¸° + ê°„ë‹¨í•œ ì „ì²˜ë¦¬
            cleaned = self.pattern.sub(' ', text)
            tokens = [token for token in cleaned.split() if len(token) > 1]
            return frozenset(tokens)

    def tokenize_simple(self, text):
        """ë‹¨ìˆœ ë„ì–´ì“°ê¸° ë¶„í•  (ê¸°ì¡´ ë°©ì‹)"""
        if pd.isna(text) or text == '':
            return frozenset()

        cleaned = self.pattern.sub(' ', str(text))
        tokens = [token for token in cleaned.split() if len(token) > 1]
        return frozenset(tokens)


# ------------------- ìµœì í™”ëœ ì „ì²˜ë¦¬ -------------------
def preprocess_data_optimized(df):
    tokenizer = HybridTokenizer()

    print("ğŸ” ì œí’ˆëª… í† í°í™” ì¤‘...")
    df['product_tokens'] = df['product'].apply(tokenizer.tokenize_smart)

    print("ğŸ” ë¦¬ë·° í† í°í™” ì¤‘...")
    # ë¦¬ë·°ëŠ” ê¸¸ì–´ì„œ ë‹¨ìˆœ ë°©ì‹ ì‚¬ìš©
    df['review_tokens'] = df['review'].apply(tokenizer.tokenize_simple)

    return df


# ------------------- Word2Vec í™•ì¥ (ê¸°ì¡´ê³¼ ë™ì¼) -------------------
@lru_cache(maxsize=128)
def expand_symptom_by_word2vec(symptom_query, embedding_model, topn_word=8):
    tokenizer = HybridTokenizer()
    tokens = list(tokenizer.tokenize_smart(symptom_query))
    expanded = set(tokens)

    for token in tokens:
        if token in embedding_model.wv:
            try:
                similar_words = [w for w, _ in embedding_model.wv.most_similar(token, topn=topn_word)]
                expanded.update(similar_words)
            except KeyError:
                continue

    return frozenset(expanded)


# ------------------- ì¶”ì²œ í•¨ìˆ˜ -------------------
def recommend_products_hybrid(df, symptom_query, embedding_model, topn=5):
    expanded_tokens = expand_symptom_by_word2vec(symptom_query, embedding_model)
    print(f"ğŸ” í™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_tokens}")

    # ë²¡í„°í™”ëœ ì ìˆ˜ ê³„ì‚°
    def calculate_score(row):
        review_score = len(expanded_tokens & row['review_tokens'])
        product_score = len(expanded_tokens & row['product_tokens']) * 2  # ì œí’ˆëª… ê°€ì¤‘ì¹˜
        return review_score + product_score

    df['temp_score'] = df.apply(calculate_score, axis=1)
    result = df[df['temp_score'] > 0].nlargest(topn, 'temp_score')
    df.drop('temp_score', axis=1, inplace=True)

    return result


# ------------------- ì‹¤í–‰ ì˜ˆì‹œ -------------------
if __name__ == "__main__":
    # ë°ì´í„° ë¡œë”©
    df_products = pd.read_csv('./models/tfidf_products.csv')
    embedding_model = Word2Vec.load('./models/word2vec_supplements_okt.model')

    # ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    df_products = preprocess_data_optimized(df_products)

    # ì¶”ì²œ ì‹¤í–‰
    symptoms = ['ê³¨ë‹¤ê³µì¦', 'ëˆˆê±´ì¡°ì¦']
    for symptom in symptoms:
        print(f"\nâœ… [{symptom}] ì¶”ì²œ ê²°ê³¼:")
        result = recommend_products_hybrid(df_products, symptom, embedding_model)
        for _, row in result.iterrows():
            print(f"- {row['product']} (ì ìˆ˜: {row.get('temp_score', 'N/A')})")