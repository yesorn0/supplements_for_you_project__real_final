from selenium import webdriver #ì›¹ ìë™í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium.webdriver.common.by import By # ìš”ì†Œ ì°¾ê¸° ë°©ì‹(id, class, xpath ë“±)
from selenium.webdriver.common.keys import Keys # í‚¤ë³´ë“œ ì…ë ¥ì„ ë‹¤ë£¨ê¸° ìœ„í•œ ëª¨ë“ˆ
from selenium.webdriver.chrome.service import Service as ChromeService # í¬ë¡¬ë“œë¼ì´ë²„ ì‹¤í–‰ ê´€ë ¨ ì„¤ì •
from selenium.webdriver.chrome.options import Options as ChromeOptions # í¬ë¡¬ ì‹¤í–‰ ì˜µì…˜ ì„¤ì •
from setuptools.package_index import user_agent
from webdriver_manager.chrome import ChromeDriverManager # ìë™ìœ¼ë¡œ í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì¹˜
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException # ì˜ˆì™¸ ì²˜ë¦¬ìš©
import pandas as pd # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ìš©
import re # ì •ê·œ í‘œí˜„ì‹ ì²˜ë¦¬ìš©
import time # ì‹œê°„ ì§€ì—°
import datetime # ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜
from selenium_stealth import stealth
from playwright.sync_api import sync_playwright
import random


# -----------------------------------------------------------
urls = [
    ('ì—¬ì„± ì¢…í•©ë¹„íƒ€ë¯¼', 'https://kr.iherb.com/c/multivitamins?_gl=1*1svu9t*_up*MQ..&cids=1783&sr=2')
    # ('ë‚¨ì„± ì¢…í•©ë¹„íƒ€ë¯¼', 'https://kr.iherb.com/c/multivitamins?_gl=1*1svu9t*_up*MQ..&cids=1782&sr=2'),
    # ('ì„ì‚°ë¶€ ì¢…í•©ë¹„íƒ€ë¯¼', 'https://kr.iherb.com/c/multivitamins?_gl=1*cciqjq*_up*MQ..&cids=100425&sr=2'),
    # ('ì•„ì—°', 'https://kr.iherb.com/c/zinc?_gl=1*jxx4ab*_up*MQ..&sr=2'),
    # ('ì…€ë ˆëŠ„', 'https://kr.iherb.com/c/selenium?_gl=1*1an9tcd*_up*MQ..&sr=2')
]

# 1. í¬ë¡¬ ì˜µì…˜ êµ¬í˜„
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.6261.95 Safari/537.36 Edg/122.0.2365.66",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.90 Safari/537.36",
]

# url ì ‘ì† í›„, ì œí’ˆì˜ id ìˆ˜ì§‘, ìˆœì°¨ì ìœ¼ë¡œ ì ‘ì†í•˜ì—¬ í¬ë¡¤ë§
results = []
with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        ua = random.choice(user_agents)  # ëœë¤ ìœ ì €ì—ì´ì „íŠ¸ ì„ íƒ
        context = browser.new_context(locale="ko-KR", user_agent=ua)
        page = context.new_page()

        # 1. ìƒí’ˆ ë§í¬ ìˆ˜ì§‘.
        product_links = []
        for name, url in urls:
                page.goto(url, timeout=20000)
                # ìƒí’ˆ ë°•ìŠ¤ ìˆ˜ì§‘: idê°€ 'pid_'ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  div
                product_divs = page.query_selector_all("div[id^='pid_']")
                for div in product_divs:
                        pid = div.get_attribute("id") # pid_125281í˜•íƒœ
                        a_tag = div.query_selector("div:nth-child(2) > div:nth-child(1) > a")
                        if a_tag:
                                href = a_tag.get_attribute("href")
                                if href:
                                        full_url = href
                                        product_links.append((pid, full_url))
                print(f"âœ… ìˆ˜ì§‘ëœ ìƒí’ˆ ìˆ˜: {len(product_links)}")
                product_links = product_links[:40]  # ìµœëŒ€ 40ê°œê¹Œì§€ë§Œ ì‚¬ìš©

        # 2. ê° ì œí’ˆ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        supplements = []  # ì˜ì–‘ì œ ì¢…ë¥˜ ì €ì¥ìš©
        product = []  # ì œí’ˆì´ë¦„ ì €ì¥ìš©
        ingredient = []  # ì˜ì–‘ì„±ë¶„ ì €ì¥ìš©
        for _, link in product_links:
                page.goto(link, timeout=20000)
                try:
                        name = page.locator('//*[@id="name"]').inner_text(timeout=3000)
                except:
                        name = "N/A"

                try:
                        overview = page.locator(
                                '//*[@id="product-overview"]/div/section/div[2]/div/div[1]/div[2]/div').inner_text(
                                timeout=3000)
                except:
                        overview = "N/A"

        # 3. ë¦¬ë·°í˜ì´ì§€ ì ‘ì† ë° ë¦¬ë·° ìˆ˜ì§‘(ìµœëŒ€ 500ê°œ ê¹Œì§€)
                review = []  # ë¦¬ë·° ì €ì¥ìš©
                base_review_url = link.replace("/pr/", "/r/")
                for page_num in range(1, 4): # í…ŒìŠ¤íŠ¸ë¡œ 3 í˜ì´ì§€ë§Œ ì‹¤í–‰
                    review_url = f"{base_review_url}?sort=6&isshowtranslated=true&p={page_num}"
                    page.goto(review_url, timeout=20000)
                    for i in range(1, 11):
                        try:
                            xpath = f'//*[@id="reviews"]/div[{i}]/div[2]/div/div[4]/a/div/div/span[1]'
                            text = page.locator(xpath).inner_text(timeout=2000)
                            review.append(text)
                        except:
                            continue
                results.append(
                        {
                                "ì˜ì–‘ì œ ì¢…ë¥˜": supplements,
                                "ì œí’ˆëª…": product,
                                "ì˜ì–‘ì„±ë¶„": ingredient,
                                "ë¦¬ë·°": review
                            })


        browser.close()

# ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ì €ì¥í•˜ê³  csvë¡œ ë‚´ë³´ë‚´ê¸°
df = pd.DataFrame(results)
df.to_csv('./crawling_data/reviews.csv', index=False) # CSVì €ì¥
print("")


# hrefs = []
# titles = []
# for i in range(1, 100):
#     # ì˜í™” ìƒì„¸ í˜ì´ì§€ ë§í¬ ì¶”ì¶œ
#     href = driver.find_element(By.XPATH,
#             '/html/body/div/div/div/main/div/div[2]/a[{}]'.format(i)).get_attribute('href')
#     hrefs.append(href)
#
#     # ì˜í™” ì œëª© ì¶”ì¶œ
#     title = driver.find_element(By.XPATH,
#             '/html/body/div/div/div/main/div/div[2]/a[{}]/div/div[2]/span'.format(i)).text
#     titles.append(title)
# print(hrefs) # ë§í¬ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
# print(titles) # ì œëª© ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
#
# # -----------------------------------------------------------
# # ë¦¬ë·° í˜ì´ì§€ ì§„ì… ë° ë‚´ìš©
#
# reviews = [] # ì „ì²´ ì˜í™” ë¦¬ë·° ì €ì¥ìš©
# for idx, url in enumerate(hrefs):
#     driver.get(url + '?tab=review') ë¦¬ë·° íƒ­ìœ¼ë¡œ ì´ë™
#     time.sleep(0.5)
#     # ë¦¬ë·°ê°€ ìˆëŠ” ì˜ì—­ ìŠ¤í¬ë¡¤
#     scroll_to_bottom(scroll_target_xpath='//*[@id="content__body"]', times=5)
#
#     review = '' # í˜„ì¬ ì˜í™”ì˜ ë¦¬ë·° ëª¨ìŒ
#     for i in range(1, 10):  # ë¦¬ë·° 9ê°œê¹Œì§€ ì‹œë„
#         try:
#             # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
#             review_xpath = '//*[@id="contents"]/div[4]/section[2]/div/article[{}]/div[3]/a[2]'.format(i)
#             review_button = driver.find_element(By.XPATH, review_xpath)
#             driver.execute_script('arguments[0].click();', review_button)
#             time.sleep(0.5)
#             try:
#                 # ìƒì„¸ ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ
#                 review = review + driver.find_element(By.XPATH,
#                       '//*[@id="contents"]/div[2]/div[1]/div/section[2]/div/div/div/p').text
#             except:
#                 # ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš° h3íƒœê·¸ì—ì„œ ê°€ì ¸ì˜´
#                 review = review + driver.find_element(By.XPATH,
#             '//*[@id="contents"]/div[2]/div[1]/div/section[2]/div/div/h3').text
#             driver.back() # ë‹¤ì‹œ ë¦¬ë·° ëª©ë¡ìœ¼ë¡œ ëŒì•„ê°
#             print(i, 'try')
#         except:
#             try:
#                 # ë”ë³´ê¸° ì—†ì´ ë°”ë¡œ ë³´ì´ëŠ” ë¦¬ë·° ìˆ˜ì •
#                 review = review + driver.find_element(By.XPATH,
#                   '//*[@id="contents"]/div[4]/section[2]/div/article[{}]/div[3]/a/h5'.format(
#                               i)).text
#                 print(review)
#                 print(i, 'NoSuchElementException')
#
#             except:
#                 # í•´ë‹¹ ìœ„ì¹˜ì— ë¦¬ë·°ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸
#                 review = review
#                 print(i, 'except')
#
#     reviews.append(review) # ë¦¬ë·° ì €ì¥
# print(reviews)  # ì „ì²´ ë¦¬ë·° ì¶œë ¥
#
# # -----------------------------------------------------------
# # ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ì €ì¥í•˜ê³  csvë¡œ ë‚´ë³´ë‚´ê¸°
# df = pd.DataFrame({'titles':titles, 'reviews':reviews}) # ì œëª©ê³¼ ë¦¬ë·°ë¡œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
# my_name = 'JKY' # ì‚¬ìš©ì ì´ë¦„
# df.to_csv('./data/reviews_{}.csv'.format(my_name), index=False) # CSVì €ì¥
#
# print("")


# # ë¸Œë¼ìš°ì € ì ‘ì† í™•ì¸ìš©
# with sync_playwright() as p:
#     browser = p.chromium.launch(headless=False)
#
#     for name, url in urls:
#         # ëœë¤ ìœ ì €ì—ì´ì „íŠ¸ ì„ íƒ
#         ua = random.choice(user_agents)
#
#         context = browser.new_context(locale="ko-KR", user_agent=ua)
#         page = context.new_page()
#
#         print(f"\nğŸŒ '{name}' ì ‘ì† ì¤‘ (User-Agent: {ua})")
#         page.goto(url, timeout=20000)
#
#         input("ğŸ‘€ í™•ì¸ í›„ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
#
#         context.close()
#
#     browser.close()

# # -----------------------------------------------------------





